{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \"\"\"NeuralNetwork Constructor.\n",
    "            @self: means an object scope method\n",
    "            @input_nodes: amount of input nodes (it is the input_array's last dimension)\n",
    "            @hidden_nodes: an int amount of neurons nodes that process the data\n",
    "            @output_nodes: an int amount of neurons nodes that process the data & retrieve the result\n",
    "            @learning_rate: the rate/size of the learning/training process (it's as lower as good)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set number of nodes in input, hidden and output layers attrs.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize the weights from input to hidden prop\n",
    "        # it's creating a matrix of numbers between 0.0 and HIDDEN amount ** -0.5, like 35 will be 0.166...\n",
    "        # the last param is setting the dimensions using HIDDEN amount as lines & INPUT amount as columns\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes ** -0.5,\n",
    "                                                        (self.hidden_nodes, self.input_nodes))\n",
    "\n",
    "        # Initialize the weights from hidden to output prop\n",
    "        # it's creating a matrix of numbers between 0.0 and OUTPUT amount ** -0.5, like 35 will be 0.166...\n",
    "        # the last param is setting the dimensions using OUPUT amount as lines & HIDDEN amount as columns        \n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes ** -0.5,\n",
    "                                                         (self.output_nodes, self.hidden_nodes))\n",
    "        \n",
    "        # set the learning rate hyperparam\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # the activation function is the neuron work\n",
    "        self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        # the activation prime is the deviration of activation and it is known as \"transfer function\"\n",
    "        self.activation_function_prime = lambda x: x*1\n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \"\"\"neuralNetworkObj.train method.\n",
    "            It method trains the NeuralNetwork to know the RESULTS (targets) based on received EVIDENCES (inputs)\n",
    "            @self: means an object scope method\n",
    "            @inputs_list: the array containing the inputs (EVIDENCES)\n",
    "            @targets_list: the array containing the results produced by the inputs\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Convert inputs list to 2d array\n",
    "#         inputs = np.array(inputs_list, ndmin=2).T\n",
    "#         targets = np.array(targets_list, ndmin=2).T\n",
    "\n",
    "        #### Implement the forward pass ####\n",
    "        # weights is (n x input_dim), (a set of weights for each hidden node)\n",
    "        # hidden inputs linear combination & generating activated output\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs.T)  # INPUT Layer generated hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)  # HIDDEN Layer prepared for output layer\n",
    "\n",
    "        # values on output layer (end nodes), same steps before (linear_comb+activation) using out_refs\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)  # HIDDEN Layer generated output entries\n",
    "        final_outputs = self.activation_function_prime(final_inputs) # the output receive it entries derivated\n",
    "\n",
    "        #### Implement the backward pass ####\n",
    "        # (y - y^)Output layer error is the difference between desired target and actual output\n",
    "        # error_term: y-y^(chapel)*ƒ’(h), ƒ’(h)=1, so error = error_term\n",
    "        output_errors = targets_list - final_outputs.T\n",
    "\n",
    "        # Backpropagated error\n",
    "        hidden_errors = output_errors * self.weights_hidden_to_output.T  # errors propagated from output to hidden\n",
    "        hidden_grad = hidden_outputs * (1 - hidden_outputs)  # hidden layer gradients from the output\n",
    "\n",
    "        # wi = wi(current_weight) + n(learning-rate)*error-term*xi(current_input) --> epoch(term) error\n",
    "        error_term = (hidden_errors * hidden_grad)\n",
    "        # propagte input to hidden\n",
    "        self.weights_input_to_hidden += self.lr * np.dot(error_term, inputs.T)        \n",
    "        # propagte hidden to output\n",
    "        self.weights_hidden_to_output += self.lr * output_errors * hidden_outputs.T \n",
    "        \n",
    "        \n",
    "    def run(self, inputs_list):\n",
    "        # Run a forward pass through the network\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "\n",
    "        #### Implement the forward pass here ####\n",
    "        # same from training function,but now using trained weights\n",
    "        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # linear combination out_weights & hidden_out\n",
    "        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs) \n",
    "        # it will return final_inputs\n",
    "        final_outputs = self.activation_function_prime(final_inputs)\n",
    "\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,2) (4,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-14464eb47e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Go through a random batch of 4 records from the training data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Printing out the training progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-844cb01f59c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs_list, targets_list)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# wi = wi(current_weight) + n(learning-rate)*error-term*xi(current_input) --> epoch(term) error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0merror_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_errors\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# propagte input to hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_input_to_hidden\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,2) (4,4) "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "### Set the hyperparameters here ###\n",
    "epochs = 300\n",
    "learning_rate = 0.05\n",
    "hidden_nodes = 4 # Neurons\n",
    "output_nodes = 1\n",
    "\n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "train_features = np.array([0, 1, 1, 0])\n",
    "val_features = np.array([0, 1, 1, 0])\n",
    "\n",
    "N_i = inputs.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for e in range(epochs):\n",
    "    # Go through a random batch of 4 records from the training data set\n",
    "    network.train(train_features, inputs)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(train_features), train_features.values)\n",
    "    val_loss = MSE(network.run(val_features), val_features.values)\n",
    "    sys.stdout.write(\"\\rProgress: \" + str(100 * e/float(epochs))[:4] \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
